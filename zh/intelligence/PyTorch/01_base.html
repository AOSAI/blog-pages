<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.14" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.50" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://mister-hope.github.io/blog-pages/zh/intelligence/PyTorch/01_base.html"><meta property="og:site_name" content="青裁的博客"><meta property="og:title" content="PyTorch基础知识"><meta property="og:description" content="1. Tensor（张量） 张量是一种特殊的数据结构，不管是 PyTorch 还是 Tensorflow 都是使用张量进行运算，因为它可以在 GPU 或其它硬件加速器上使用，并且针对自动微分进行了优化。 它有点类似 Numpy 的 ndarray，并且很多张量 API 的使用方式都是相同的，如果你熟悉 Numpy，你会发现 torch 使用起来也会非常..."><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="AOSAI"><meta property="article:tag" content="PyTorch基础"><meta property="article:published_time" content="2024-09-13T00:00:00.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"PyTorch基础知识","image":[""],"datePublished":"2024-09-13T00:00:00.000Z","dateModified":null,"author":[{"@type":"Person","name":"AOSAI"}]}</script><title>PyTorch基础知识 | 青裁的博客</title><meta name="description" content="1. Tensor（张量） 张量是一种特殊的数据结构，不管是 PyTorch 还是 Tensorflow 都是使用张量进行运算，因为它可以在 GPU 或其它硬件加速器上使用，并且针对自动微分进行了优化。 它有点类似 Numpy 的 ndarray，并且很多张量 API 的使用方式都是相同的，如果你熟悉 Numpy，你会发现 torch 使用起来也会非常...">
    <link rel="preload" href="/blog-pages/assets/style-ChcH9QVm.css" as="style"><link rel="stylesheet" href="/blog-pages/assets/style-ChcH9QVm.css">
    <link rel="modulepreload" href="/blog-pages/assets/app-CH05aeyc.js"><link rel="modulepreload" href="/blog-pages/assets/01_base.html-DUKVIMxW.js"><link rel="modulepreload" href="/blog-pages/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/blog-pages/assets/index.html-CM-_Du5R.js" as="script"><link rel="prefetch" href="/blog-pages/assets/disable.html-CPKt6g_o.js" as="script"><link rel="prefetch" href="/blog-pages/assets/encrypt.html-CP64n5eQ.js" as="script"><link rel="prefetch" href="/blog-pages/assets/layout.html-D7vYLCVY.js" as="script"><link rel="prefetch" href="/blog-pages/assets/markdown.html-D26m9E54.js" as="script"><link rel="prefetch" href="/blog-pages/assets/page.html-BEVMPRmq.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-BCXAj-QM.js" as="script"><link rel="prefetch" href="/blog-pages/assets/cherry.html-CkE9Lo-3.js" as="script"><link rel="prefetch" href="/blog-pages/assets/dragonfruit.html-k9uQKkD1.js" as="script"><link rel="prefetch" href="/blog-pages/assets/strawberry.html-CPa5dFZn.js" as="script"><link rel="prefetch" href="/blog-pages/assets/tomato.html-BRkD9429.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-DoFgjsgp.js" as="script"><link rel="prefetch" href="/blog-pages/assets/1.html-UCYUDBb2.js" as="script"><link rel="prefetch" href="/blog-pages/assets/2.html-h1R-j8xA.js" as="script"><link rel="prefetch" href="/blog-pages/assets/3.html-BpLBlAWE.js" as="script"><link rel="prefetch" href="/blog-pages/assets/4.html-CG6e7sPY.js" as="script"><link rel="prefetch" href="/blog-pages/assets/1.html-G45bXKIo.js" as="script"><link rel="prefetch" href="/blog-pages/assets/2.html-BQD7mkJ-.js" as="script"><link rel="prefetch" href="/blog-pages/assets/3.html-BgEcG7JF.js" as="script"><link rel="prefetch" href="/blog-pages/assets/4.html-BkG4XOMc.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-S8NVGnzx.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-CleEgZIq.js" as="script"><link rel="prefetch" href="/blog-pages/assets/01_fargoing.html-DDz5H7n6.js" as="script"><link rel="prefetch" href="/blog-pages/assets/02_sichuan.html-C-7wook4.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-Be-PwI-X.js" as="script"><link rel="prefetch" href="/blog-pages/assets/01.html-qSCtWpRI.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-CCP1RVp3.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-DX4rRa3K.js" as="script"><link rel="prefetch" href="/blog-pages/assets/02_linear_regression.html-BgnTlc_T.js" as="script"><link rel="prefetch" href="/blog-pages/assets/03_linear_regression.html-D5UjEQNw.js" as="script"><link rel="prefetch" href="/blog-pages/assets/04_classification.html-rkkjgJ5j.js" as="script"><link rel="prefetch" href="/blog-pages/assets/05_deep_learning.html-BKE-cIXF.js" as="script"><link rel="prefetch" href="/blog-pages/assets/06_tensorflow.html-C6-ap8zw.js" as="script"><link rel="prefetch" href="/blog-pages/assets/07_model_evaluation.html-B3RPlCVH.js" as="script"><link rel="prefetch" href="/blog-pages/assets/08_decision_tree.html-Cu5UYSDV.js" as="script"><link rel="prefetch" href="/blog-pages/assets/09_unsupervised_learning.html-ugM8icHR.js" as="script"><link rel="prefetch" href="/blog-pages/assets/10_recommendation_system.html-CSwl-duD.js" as="script"><link rel="prefetch" href="/blog-pages/assets/11_reinforcement_learning.html-BUyOjDiY.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-DECXjF33.js" as="script"><link rel="prefetch" href="/blog-pages/assets/01.html-DYVEGu6z.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-Dovvm0lH.js" as="script"><link rel="prefetch" href="/blog-pages/assets/01.html-RsTecOfy.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-Dex3v3T1.js" as="script"><link rel="prefetch" href="/blog-pages/assets/02_project1.html-Ccn8MrpU.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-DEI-aI5L.js" as="script"><link rel="prefetch" href="/blog-pages/assets/01_recommend_model.html-B0ySzQI7.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-BLE89CqR.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-C9NqB5rf.js" as="script"><link rel="prefetch" href="/blog-pages/assets/01_compress_photo.html-Dbpyf09_.js" as="script"><link rel="prefetch" href="/blog-pages/assets/02_file_packaging.html-6PV-Xydy.js" as="script"><link rel="prefetch" href="/blog-pages/assets/03_pyqt5_recording.html-CqBAru2J.js" as="script"><link rel="prefetch" href="/blog-pages/assets/04_spider_for_ticket.html-DyCHMUjv.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-OparkUpZ.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-CWIwhSh4.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-DVtd6pll.js" as="script"><link rel="prefetch" href="/blog-pages/assets/404.html-BsKWPMmJ.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-BRMvZ0Ll.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-tWZpuTrU.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-D90bFmxr.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-DqRPj--6.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-D7_mBELA.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-BDtR3x3e.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-ftNcK0d-.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-BikQiGUT.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-nt-vZbJO.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-DKOGH_y_.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-IK2TdfhV.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html--TOjr6g4.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-D0Ib1QSj.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-B4dq7NG_.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-4ER5OxdC.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-CQ2CEfG8.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-DZadTeIz.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-D3i_jQOs.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-Hj5LUVFx.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-BXgxVcsT.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-CQH-aOo3.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-DOtV51Fn.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-CWZuOZp6.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-DbVzWGP1.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-CzKA9ZQV.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-BVDRYuBL.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-_hrAIzP7.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-CEmIPmgX.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-DmC1ts_n.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-Vhu-cRC6.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-foGaHCBQ.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-BOILAONN.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-C5YoCZRY.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-Bn9Bfsq6.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-6nLgtfEh.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-XcsOz3yt.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-CiD-wsng.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-DHrzAxbt.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-4UV4l_ip.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-Vokv3-Zg.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-CkZ8nsRs.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-BzrB1X5U.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-DyEmxGRD.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-CMXEaJUG.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-Cw2AnjIJ.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-CSjmCj9d.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-D6Cfgbgm.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-CqMY5xds.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-BvSkESQN.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-DP3gd-oE.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-Dx3qC1L8.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-AvgKz32o.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-DL0Sp0w-.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-jgDNeDf8.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-Dg8TGTeK.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-Di4-3yio.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-qvMPjqCA.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-B6-KbPl9.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-dP3-8QKs.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-NC0STgEF.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-DKGFJS7X.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-CWY0j3cn.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-COyu5rOG.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-Dft4fa9W.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-CcrE5-PT.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-BLAysKiR.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-C6nMWlnC.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-Cdhub77P.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-Baz58OQN.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-BaWK0uc2.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-D0Cqn1bm.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-Cz9sWoN9.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-Bdi6fkv9.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-DpDWTb3C.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-BW5_GDRT.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-BQpCtJoH.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-Bu6sE2qX.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-D_uuS58B.js" as="script"><link rel="prefetch" href="/blog-pages/assets/index.html-Bf6Iyqfr.js" as="script"><link rel="prefetch" href="/blog-pages/assets/photoswipe.esm-GXRgw7eJ.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container external-link-icon has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><!--[--><a class="route-link vp-brand" href="/blog-pages/zh/"><img class="vp-nav-logo" src="/blog-pages/blog_logo.svg" alt><!----><span class="vp-site-name hide-in-pad">青裁的博客</span></a><!--]--><!----></div><div class="vp-navbar-center"><!----><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/blog-pages/zh/" aria-label="博客主页"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span><!--]-->博客主页<!----></a></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="软件开发"><!--[--><!---->软件开发<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog-pages/zh/software/pyqt5/" aria-label="PyQt5"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-sitemap" style=""></span><!--]-->PyQt5<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog-pages/zh/software/front_end/" aria-label="前端开发"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-sitemap" style=""></span><!--]-->前端开发<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog-pages/zh/software/back_end/" aria-label="后端开发"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-sitemap" style=""></span><!--]-->后端开发<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog-pages/zh/software/desktop_app/" aria-label="桌面应用"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-sitemap" style=""></span><!--]-->桌面应用<!----></a></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="人工智能"><!--[--><!---->人工智能<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog-pages/zh/intelligence/MachineLearning/" aria-label="机器学习"><!---->机器学习<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog-pages/zh/intelligence/Numpy/" aria-label="Numpy"><!---->Numpy<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog-pages/zh/intelligence/Matplotlib/" aria-label="Matplotlib"><!---->Matplotlib<!----></a></li><li class="vp-dropdown-item"><a class="route-link route-link-active auto-link" href="/blog-pages/zh/intelligence/PyTorch/" aria-label="PyTorch"><!---->PyTorch<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog-pages/zh/intelligence/recommendationSystem/" aria-label="推荐系统"><!---->推荐系统<!----></a></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="玩出点名堂"><!--[--><!---->玩出点名堂<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog-pages/zh/dobetter/mahjong/" aria-label="麻将秘籍"><!---->麻将秘籍<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog-pages/zh/dobetter/musictheroy/" aria-label="吉他乐理"><!---->吉他乐理<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog-pages/zh/dobetter/photograph/" aria-label="摄影摄像"><!---->摄影摄像<!----></a></li></ul></button></div></div></nav><!--]--><!----></div><div class="vp-navbar-end"><!----><!--[--><div class="vp-nav-item"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="选择语言"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon i18n-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="i18n icon" name="i18n" style="width:1rem;height:1rem;vertical-align:middle;"><path d="M379.392 460.8 494.08 575.488l-42.496 102.4L307.2 532.48 138.24 701.44l-71.68-72.704L234.496 460.8l-45.056-45.056c-27.136-27.136-51.2-66.56-66.56-108.544h112.64c7.68 14.336 16.896 27.136 26.112 35.84l45.568 46.08 45.056-45.056C382.976 312.32 409.6 247.808 409.6 204.8H0V102.4h256V0h102.4v102.4h256v102.4H512c0 70.144-37.888 161.28-87.04 210.944L378.88 460.8zM576 870.4 512 1024H409.6l256-614.4H768l256 614.4H921.6l-64-153.6H576zM618.496 768h196.608L716.8 532.48 618.496 768z"></path></svg><!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog-pages/" aria-label="English"><!---->English<!----></a></li><li class="vp-dropdown-item"><a class="route-link route-link-active auto-link" href="/blog-pages/zh/intelligence/PyTorch/01_base.html" aria-label="简体中文"><!---->简体中文<!----></a></li></ul></button></div></div><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/vuepress-theme-hope/vuepress-theme-hope" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-color-mode-switch" id="color-mode-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" name="auto" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" name="dark" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" name="light" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!----><!--]--><!----><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!----><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/blog-pages/zh/intelligence/PyTorch/" aria-label="PyTorch入门手册"><!---->PyTorch入门手册<!----></a></li><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/blog-pages/zh/intelligence/PyTorch/01_base.html" aria-label="PyTorch基础知识"><!---->PyTorch基础知识<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog-pages/zh/intelligence/PyTorch/02_project1.html" aria-label="实战1-线性回归"><!---->实战1-线性回归<!----></a></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->PyTorch基础知识</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">AOSAI</span></span><span property="author" content="AOSAI"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2024-09-13T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 10 分钟</span><meta property="timeRequired" content="PT10M"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item color3 clickable" role="navigation">PyTorch</span><!--]--><meta property="articleSection" content="PyTorch"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item color5 clickable" role="navigation">PyTorch基础</span><!--]--><meta property="keywords" content="PyTorch基础"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc"><!----><div class="vp-toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon" name="print"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_1-tensor-张量">1. Tensor（张量）</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_1-1-初始化">1.1 初始化</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_1-2-维度及属性">1.2 维度及属性</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_1-3-张量上的操作">1.3 张量上的操作</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_1-4-数学运算">1.4 数学运算</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_1-5-与-numpy-的桥梁">1.5 与 Numpy 的桥梁</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_2-pytorch-常用封装">2. PyTorch 常用封装</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2-1-数据加载器-数据集">2.1 数据加载器（数据集）</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2-2-数据变换">2.2 数据变换</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2-3-神经网络模型构建">2.3 神经网络模型构建</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2-4-自动微分-求导">2.4 自动微分（求导）</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2-5-参数优化">2.5 参数优化</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2-6-模型的保存和加载">2.6 模型的保存和加载</a></li><!----><!--]--></ul></li><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!----></aside></div><!----><div class="theme-hope-content"><h2 id="_1-tensor-张量" tabindex="-1"><a class="header-anchor" href="#_1-tensor-张量"><span>1. Tensor（张量）</span></a></h2><p>张量是一种特殊的数据结构，不管是 PyTorch 还是 Tensorflow 都是使用张量进行运算，因为它可以在 GPU 或其它硬件加速器上使用，并且针对自动微分进行了优化。</p><p>它有点类似 Numpy 的 ndarray，并且很多张量 API 的使用方式都是相同的，如果你熟悉 Numpy，你会发现 torch 使用起来也会非常顺手，而且它两是可以相互转化的。</p><h3 id="_1-1-初始化" tabindex="-1"><a class="header-anchor" href="#_1-1-初始化"><span>1.1 初始化</span></a></h3><p>PyTorch 创建张量的 API 很多，这里只写一些比较常见的方式，想要了解的更详细，请看相关链接。首先，导入 torch 和 numpy：</p><div class="language-py line-numbers-mode" data-highlighter="shiki" data-ext="py" data-title="py" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> numpy </span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> np</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><ol><li><strong>直接从 Python 数据初始化</strong>，数据类型 torch 会自动推断。</li><li><strong>从 Numpy 数组初始化</strong>，Numpy 的 ndarray 可以和 PyTorch 的 Tensor 相互转化。</li><li><strong>从另一个张量初始化</strong>，除非显示覆盖，否则新张量将保留参数张量的属性（形状、数据类型）。</li></ol><div class="language-py line-numbers-mode" data-highlighter="shiki" data-ext="py" data-title="py" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># 初始化一个名叫 data 的 python 列表，并转化为张量形式</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">data </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> [[</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">],[</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">]]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">x_data </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">tensor</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(data)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># 将 data 转化为 ndarray，再将 ndarray 转化为 tensor</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">np_array </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> np.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">array</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(data)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">x_np </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">from_numpy</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(np_array)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># 以 x_data 的形式，创建一个全为 1 的张量</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">x_ones </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">ones_like</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(x_data)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># 以 x_data 的形式，创建一个由随机数组成的张量</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">x_rand </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">rand_like</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(x_data, </span><span style="--shiki-light:#E36209;--shiki-dark:#E06C75;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">torch.float)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;Random Tensor: </span><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;"> {</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">x_rand</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;"> \n</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol start="4"><li><strong>使用随机值或常量值等</strong>，shape 是一个张量维度的元组，也就是上面所说的张量的形状属性。</li></ol><div class="language-py line-numbers-mode" data-highlighter="shiki" data-ext="py" data-title="py" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># 定义一个二维的，2行3列 的张量形状</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">shape </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">,)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># 以这个张量形状，别分创建由 随机数、全为1、全为0 组成的张量</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">rand_tensor </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">rand</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(shape)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">ones_tensor </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">ones</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(shape, </span><span style="--shiki-light:#E36209;--shiki-dark:#E06C75;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">torch.long)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">zeros_tensor </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">zeros</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(shape, </span><span style="--shiki-light:#E36209;--shiki-dark:#E06C75;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">torch.float32)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;Random Tensor: </span><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;"> {</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">rand_tensor</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;"> \n</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>参考文献&amp;相关链接：</strong></p><ol><li><a href="https://pytorch.ac.cn/docs/stable/torch.html#creation-ops" target="_blank" rel="noopener noreferrer">《PyTorch 中文文档 - 张量创建相关 API》</a></li><li><a href="https://pytorch.ac.cn/tutorials/beginner/basics/tensorqs_tutorial.html" target="_blank" rel="noopener noreferrer">《PyTorch 中文文档 - 张量简介》</a></li></ol><h3 id="_1-2-维度及属性" tabindex="-1"><a class="header-anchor" href="#_1-2-维度及属性"><span>1.2 维度及属性</span></a></h3><ul><li>0 维：scalar（数值、标量）</li><li>1 维：vector（向量、一维向量）</li><li>2 维：matrix（矩阵、二维向量）</li><li>n 维：n-dimensional tensor（n 维向量）</li></ul><div class="language-py line-numbers-mode" data-highlighter="shiki" data-ext="py" data-title="py" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">x0 </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;"> tensor</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">42</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">.)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">x1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;"> tensor</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">([</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1.1</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1.2</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1.3</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">x2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;"> tensor</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1.1</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1.2</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1.3</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">], [</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">2.1</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">2.2</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">2.3</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">]])</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>张量的属性有很多，只列举一部分。但是支持初始化时传参的，仅有前三个：</p><ol><li>init：仅在 init 初始化构造器中使用，支持传入 initializer 的子类</li><li>形状（shape）：是一个由数字组成的元组（tuple）</li><li>数据类型（dtype）：double、float、long、boolean 等</li></ol><ul><li>转置（T）：线性代数里有讲</li><li>单个元素大小（itemsize）：整数，代表每一个元素占用的字节数</li><li>总的字节数量（nbytes）：整数，代表 Tensor 占用的总字节数</li><li>维度数量（ndim）：整数，代表 Tensor 的秩，=len(tensor.shape)</li><li>元素个数（size）：整数，表示一共有多少个元素</li><li>每一维度步长（strides）：元组（tuple），每一个维度所需要的字节数</li><li>储存张量的设备（device）：cpu、gpu</li></ul><div class="language-py line-numbers-mode" data-highlighter="shiki" data-ext="py" data-title="py" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">x1.ndim   </span><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># 1</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">x1.shape  </span><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># (1, )</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">x2.T      </span><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># [[1.1, 2.1], [1.2, 2.2], [1.3, 2.3]]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_1-3-张量上的操作" tabindex="-1"><a class="header-anchor" href="#_1-3-张量上的操作"><span>1.3 张量上的操作</span></a></h3><p>张量上的操作超过了 100 种，包括算数、线性代数、矩阵操作、采样等等。这些操作都可以运行在 GPU 上，通常比 CPU 上的速度更快。</p><p>但是默认情况下张量是在 CPU 上创建的，我们需要通过 .to 的方法将张量显式的转移到 GPU 上去。</p><div class="language-py line-numbers-mode" data-highlighter="shiki" data-ext="py" data-title="py" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># 如果 GPU 可用</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">if</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.cuda.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">is_availabel</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">():</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;">  # 将数据发送给 GPU，并重新赋值</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">  tensor </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> tensor.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">to</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;cuda&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># 也可以通过 device() 指定设备</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">device </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">device</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;cpu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># 如果 GPU 可用选择 GPU，否则选择 CPU</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">device </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">device</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;cuda&quot;</span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;"> if</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.cuda.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">is_availabel</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">() </span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">else</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;"> &quot;cpu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">tensor </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> tensor.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">to</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(device)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>索引和切片操作</strong>，与 Numpy、Python 并无不同：</p><div class="language-py line-numbers-mode" data-highlighter="shiki" data-ext="py" data-title="py" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">tensor </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">ones</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;First row: </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">tensor[</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">]</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;First column: </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">tensor[:, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">]</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;Last column: </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">tensor[</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">]</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">tensor[:,</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;"> 0</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(tensor)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>张量连接操作</strong>，可以使用 torch.cat 和 torch.stack 对张量进行连接：</p><div class="language-py line-numbers-mode" data-highlighter="shiki" data-ext="py" data-title="py" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">tensor </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">ones</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(tensor)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">t1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">cat</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">([tensor, tensor, tensor], </span><span style="--shiki-light:#E36209;--shiki-dark:#E06C75;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;">dim</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(t1)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>单元素张量（数值）</strong>，比如你做了求和操作，可以使用 item() 函数将其从张量数据转化为 Python 数据：</p><div class="language-py line-numbers-mode" data-highlighter="shiki" data-ext="py" data-title="py" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">tensor </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">ones</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">agg </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> tensor.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">sum</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">agg_item </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> agg.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">item</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">()</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(agg_item, </span><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(agg_item))</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>参考文献&amp;相关链接：</strong></p><ol><li><a href="https://pytorch.ac.cn/docs/stable/torch.html#indexing-slicing-joining-mutating-ops" target="_blank" rel="noopener noreferrer">《PyTorch 中文文档 - 张量操作及运算 API》</a></li><li><a href="https://blog.csdn.net/Ethan_Rich/article/details/134799695" target="_blank" rel="noopener noreferrer">《Pytorch 指定设备》</a></li><li><a href="https://pytorch.ac.cn/tutorials/beginner/basics/tensorqs_tutorial.html#operations-on-tensors" target="_blank" rel="noopener noreferrer">《PyTorch 中文文档 - 张量上的操作》</a></li></ol><h3 id="_1-4-数学运算" tabindex="-1"><a class="header-anchor" href="#_1-4-数学运算"><span>1.4 数学运算</span></a></h3><p>Tensor 里简单的运算，比如加减乘除、取余（%）、整除（//），可以使用 Python 中的运算符，也可以使用封装好的运算函数。</p><p><strong>逐点运算</strong>：这些基本运算都是对数值的运算，放在 1 维以上的维度中，就是相同位置元素之间的运算，所以两个张量的形状必须一样。</p><div class="language-py line-numbers-mode" data-highlighter="shiki" data-ext="py" data-title="py" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">x </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">ones</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> ,</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">y </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">ones</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">z1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">+</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> y</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">z2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> y</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">z3 </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> y</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">z4 </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> y</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(x, </span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, y, </span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, z1, </span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, z2, </span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, z3, </span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, z4)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">m1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">add</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(x, y)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">m2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">sub</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(x, y)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">m3 </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">mul</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(x, y)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">m4 </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">div</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(x, y)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(m1, </span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, m2, </span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, m3, </span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, m4)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>矩阵运算</strong>：在线性代数中，最常用的就是矩阵（向量）的乘法、转置、求逆等操作：</p><div class="language-py line-numbers-mode" data-highlighter="shiki" data-ext="py" data-title="py" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">a </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">tensor</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">([</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">.,</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">.])   </span><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># 向量默认都是竖向量</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">b </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">tensor</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">([</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">.,</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">.]).</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">view</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)   </span><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># 通过 view 变换成横向量</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># 二维矩阵中的矩阵乘法有这 3 种形式</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">mm</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(a, b))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">matmul</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(a, b))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(a </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> b)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># 假如参与运算的是一个多维张量，那么只有torch.matmul()可以使用</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># 并且在多维张量中，参与矩阵运算的只有后两个维度，前面的维度就像是索引一样</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">a </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">rand</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">((</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">64</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">32</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">b </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">rand</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">((</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">32</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">64</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">matmul</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(a, b).shape)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">&gt;&gt;&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">Size</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">([</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">64</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">64</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">])</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>参考文献&amp;相关链接：</strong></p><ol><li><a href="https://pytorch.ac.cn/docs/stable/torch.html#math-operations" target="_blank" rel="noopener noreferrer">《PyTorch 中文文档 - 数学运算 API》</a></li><li><a href="https://blog.csdn.net/qq_40728667/article/details/134013899" target="_blank" rel="noopener noreferrer">《PyTorch 中的常见运算》</a></li></ol><h3 id="_1-5-与-numpy-的桥梁" tabindex="-1"><a class="header-anchor" href="#_1-5-与-numpy-的桥梁"><span>1.5 与 Numpy 的桥梁</span></a></h3><p><strong>torch 张量变换为 Numpy 数组：</strong></p><div class="language-py line-numbers-mode" data-highlighter="shiki" data-ext="py" data-title="py" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">t </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">ones</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">n </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> t.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">numpy</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">()</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;t: </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">t</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">n: </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">n</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># 此时如果操作张量 t，numpy 数组同样也会变化</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">t.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">add_</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;t: </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">t</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">n: </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">n</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>Numpy 数组变换为 torch 张量：</strong></p><div class="language-py line-numbers-mode" data-highlighter="shiki" data-ext="py" data-title="py" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">n </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> np.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">ones</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">t </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">from_numpy</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(n)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;n: </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">n</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">t: </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">t</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># 此时如果操作 numpy 数组，张量 t 同样也会变化</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">np.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">add</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(n, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#E06C75;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;">out</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">n)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;n: </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">n</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">t: </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">t</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_2-pytorch-常用封装" tabindex="-1"><a class="header-anchor" href="#_2-pytorch-常用封装"><span>2. PyTorch 常用封装</span></a></h2><p>这里列举了一些，做模型训练时经常会用到的函数，或者说 API。仅仅只是介绍一下，有个印象，具体的用法和实例我会在每一个部分都添加几个比较易懂的博文。</p><h3 id="_2-1-数据加载器-数据集" tabindex="-1"><a class="header-anchor" href="#_2-1-数据加载器-数据集"><span>2.1 数据加载器（数据集）</span></a></h3><p>处理数据样本的代码可能会变得混乱且难以维护，理想的情况下，我们希望数据集代码与模型训练代码分离开来，让其可以方便的模块化、以及提高可读性。</p><p>所以 PyTorch 给我们提供了两个处理数据集的 API：</p><ul><li><p>torch.utils.data.Dataset：用于处理单个训练样本，读取数据特征、size、标签等，并且包括数据转换等；</p></li><li><p>torch.utils.data.DataLoader：DataLoader 在 Dataset 周围重载一个可迭代对象，以便轻松访问样本。</p></li></ul><p><strong>参考文献&amp;相关链接：</strong></p><ol><li><a href="https://blog.csdn.net/weixin_47748259/article/details/135611161" target="_blank" rel="noopener noreferrer">Dataset 与 DataLoader 使用、构建自定义数据集</a></li><li><a href="https://pytorch.ac.cn/tutorials/beginner/basics/data_tutorial.html" target="_blank" rel="noopener noreferrer">PyTorch 中文文档 - 数据集 &amp; 数据加载器</a></li></ol><h3 id="_2-2-数据变换" tabindex="-1"><a class="header-anchor" href="#_2-2-数据变换"><span>2.2 数据变换</span></a></h3><p>一般情况下，预加载的数据集或自己构造的数据集并不能直接用于训练机器学习算法，为了将其转换为训练模型所需的最终形式，我们可以使用 torchvision.transforms 对数据进行处理，以使其适合训练。</p><p>从包名我们就可以看出来，这是一个专门为了计算机视觉（图像）处理而写的 API，不做 CV 的人可以跳过。</p><p><strong>参考文献&amp;相关链接：</strong></p><ol><li><a href="https://blog.csdn.net/weixin_41936775/article/details/117160981" target="_blank" rel="noopener noreferrer">Pytorch(三)：数据变换 Transforms</a></li><li><a href="https://pytorch.ac.cn/tutorials/beginner/basics/transforms_tutorial.html" target="_blank" rel="noopener noreferrer">PyTorch 中文文档 - 变换</a></li></ol><h3 id="_2-3-神经网络模型构建" tabindex="-1"><a class="header-anchor" href="#_2-3-神经网络模型构建"><span>2.3 神经网络模型构建</span></a></h3><p>怎么说呢，就是一些简单的机器学习的问题，比如线性回归、逻辑回归等，都可以使用神经网络的形式去进行解决，并且实现层面也比较简单，所以推荐直接从神经网络开始上手。如果有原理什么不懂的，可以查看我的《机器学习》的博文，或者百度。</p><p>PyTorch 里面，neural network 直接被简写成 nn，非常的简洁。 torch.nn 命名空间提供了构建您自己的神经网络所需的所有构建块。PyTorch 中的每个模块都是 nn.Module 的子类。神经网络本身就是一个模块，它由其他模块（层）组成。这种嵌套结构允许轻松构建和管理复杂的架构。</p><p><strong>参考文献&amp;相关链接：</strong></p><ol><li><a href="https://blog.csdn.net/AI_dataloads/article/details/133144350" target="_blank" rel="noopener noreferrer">神经网络模型（最细的手写字识别案例）</a></li><li><a href="https://pytorch.ac.cn/tutorials/beginner/basics/buildmodel_tutorial.html" target="_blank" rel="noopener noreferrer">PyTorch 中文文档 - 构建神经网络</a></li></ol><h3 id="_2-4-自动微分-求导" tabindex="-1"><a class="header-anchor" href="#_2-4-自动微分-求导"><span>2.4 自动微分（求导）</span></a></h3><p>第一次跟着 b 站博主写鸢尾花分类问题的代码时，我还不太理解为什么会有<strong>反向传播</strong>这个操作，而且每次都要做，每一个迭代还都要清零一次。</p><p>我们知道神经网络，是从输入层，到隐藏层（1 to n），再到输出层，这是一步一步向前走的，叫做<strong>向前传播</strong>，在继承 nn.Module 的类时，必须要重写的一个类函数就是它，def forward(self)这样子。</p><p><strong>反向传播</strong>顾名思义，就是从后往前走，主要是<a href="https://aosai.github.io/blog-pages/zh/intelligence/MachineLearning/02_linear_regression.html#_3-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-gradient-descent" target="_blank" rel="noopener noreferrer">梯度下降</a>时需要用到。为了计算这些梯度，PyTorch 有一个内置的微分引擎，称为 torch.autograd。它支持自动计算任何计算图的梯度。只需要在张量初始化的时候，加入一个属性：requires_grad=True 即可。</p><p><strong>参考文献&amp;相关链接：</strong></p><ol><li><a href="https://blog.csdn.net/qq_35812205/article/details/120814447" target="_blank" rel="noopener noreferrer">【PyTorch 基础教程 4】反向传播与计算图（学不会来打我啊）</a></li><li><a href="https://pytorch.ac.cn/tutorials/beginner/basics/autogradqs_tutorial.html" target="_blank" rel="noopener noreferrer">PyTorch 中文文档 - 使用 torch.autograd 进行自动微分</a></li></ol><h3 id="_2-5-参数优化" tabindex="-1"><a class="header-anchor" href="#_2-5-参数优化"><span>2.5 参数优化</span></a></h3><p>这里的参数优化主要是指<strong>超参数</strong>，它是可调整的参数，允许您控制模型优化过程。不同的超参数值会影响模型训练和收敛速度。</p><p>经过吴恩达教授的机器学习课程，我们也积累的很多类型的超参数，比如：</p><ul><li>线性回归里的：学习率 alpha（α）</li><li>逻辑回归里的：正则化参数 lambda（λ）</li><li>神经网络里的：迭代训练次数、批量大小</li><li>......</li></ul><p>PyTorch 内置了很多类型的参数优化器，比如 SGD 优化器，ADAM 优化器，RMSProp 优化器等等，它们适用于不同类型的模型和数据。</p><p><strong>参考文献&amp;相关链接：</strong></p><ol><li><a href="https://blog.csdn.net/weixin_46649052/article/details/119718582" target="_blank" rel="noopener noreferrer">PyTorch 学习—13.优化器 optimizer 的概念及常用优化器</a></li><li><a href="https://pytorch.ac.cn/tutorials/beginner/basics/optimization_tutorial.html" target="_blank" rel="noopener noreferrer">PyTorch 中文文档 - 优化模型参数</a></li></ol><h3 id="_2-6-模型的保存和加载" tabindex="-1"><a class="header-anchor" href="#_2-6-模型的保存和加载"><span>2.6 模型的保存和加载</span></a></h3><p>保存模型就是为了再次使用，不管我们是在这个保存的数据基础上进一步的训练优化，还是我们去做迁移学习、共享参数，我们都得先把这个训练好的模型记录下来。</p><p><strong>参考文献&amp;相关链接：</strong></p><ol><li><a href="https://blog.csdn.net/m0_52987303/article/details/136509035" target="_blank" rel="noopener noreferrer">PyTorch 中的模型保存：一键保存、两种选择/保存整个模型和保存模型参数</a></li><li><a href="https://blog.csdn.net/qq_39698985/article/details/141823143" target="_blank" rel="noopener noreferrer">pytorch 模型保存及加载参数恢复训练的例子</a></li><li><a href="https://pytorch.ac.cn/tutorials/beginner/basics/saveloadrun_tutorial.html" target="_blank" rel="noopener noreferrer">PyTorch 中文文档 - 保存和加载模型</a></li></ol></div><!----><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="auto-link external-link vp-meta-label" href="https://github.com/vuepress-theme-hope/vuepress-theme-hope/edit/main/src/zh/intelligence/PyTorch/01_base.md" aria-label="在 GitHub 上编辑此页" rel="noopener noreferrer" target="_blank"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->在 GitHub 上编辑此页<!----></a></div><div class="vp-meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav"><a class="route-link route-link-active auto-link prev" href="/blog-pages/zh/intelligence/PyTorch/" aria-label="PyTorch入门手册"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><!---->PyTorch入门手册</div></a><a class="route-link auto-link next" href="/blog-pages/zh/intelligence/PyTorch/02_project1.html" aria-label="实战1-线性回归"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">实战1-线性回归<!----></div></a></nav><!----><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">等我攒够了六便士，就去寻找月亮</div><div class="vp-copyright">Copyright © 2024 AOSAI </div></footer></div><!--]--><!--[--><!----><!--]--><!--]--></div>
    <script type="module" src="/blog-pages/assets/app-CH05aeyc.js" defer></script>
  </body>
</html>
